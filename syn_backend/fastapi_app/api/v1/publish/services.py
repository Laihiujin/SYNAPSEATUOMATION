"""
å‘å¸ƒæ¨¡å—ä¸šåŠ¡é€»è¾‘å±‚
é›†æˆä»»åŠ¡é˜Ÿåˆ—ã€é¢„è®¾ç®¡ç†ã€æ–‡ä»¶éªŒè¯ã€è´¦å·éªŒè¯
"""
import sys
import json
import uuid
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
import asyncio
import warnings

from sqlalchemy import text

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent.parent))

from myUtils.task_queue_manager import TaskQueueManager, Task, TaskType, TaskStatus
from myUtils.preset_manager import PresetManager
from myUtils.cookie_manager import cookie_manager
from myUtils.platform_metadata_adapter import format_metadata_for_platform
from fastapi_app.core.logger import logger
from fastapi_app.core.exceptions import NotFoundException, BadRequestException
from fastapi_app.db.runtime import mysql_enabled, sa_connection
from platforms.path_utils import resolve_video_file


class PublishService:
    """å‘å¸ƒæœåŠ¡"""

    # å¹³å°ä»£ç æ˜ å°„
    PLATFORM_MAP = {
        1: "xiaohongshu",
        2: "channels",
        3: "douyin",
        4: "kuaishou",
        5: "bilibili"
    }

    def __init__(self, task_manager: TaskQueueManager):
        self.task_manager = task_manager
        self.preset_manager = PresetManager()
        self.executor = ThreadPoolExecutor(max_workers=3)

    def _portable_video_path(self, raw: str) -> str:
        """
        Make the stored `file_path` resilient to repo moves.

        If DB stores an absolute path that no longer exists (e.g. D:\\... from an old location),
        enqueue a portable filename/relative path so `resolve_video_file()` can map it on the worker host.
        """
        if not raw:
            return raw
        try:
            p = Path(str(raw))
            if p.exists():
                return str(p)
            resolved = resolve_video_file(str(raw))
            if resolved and Path(resolved).exists():
                return str(resolved)
            # If absolute but missing, enqueue basename so resolver can find it under syn_backend/videoFile.
            if p.is_absolute():
                return p.name
        except Exception:
            pass
        return str(raw)

    async def validate_file(self, db, file_id: int) -> Dict[str, Any]:
        """éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        if mysql_enabled():
            warnings.warn("SQLite publish/file path is deprecated; using MySQL via DATABASE_URL", DeprecationWarning)
            with sa_connection() as conn:
                row = conn.execute(
                    text("SELECT * FROM file_records WHERE id = :id"),
                    {"id": file_id},
                ).mappings().first()
            if not row:
                raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨ ID {file_id}")
            return dict(row)

        cursor = db.cursor()
        cursor.execute("SELECT * FROM file_records WHERE id = ?", (file_id,))
        row = cursor.fetchone()

        if not row:
            raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨: ID {file_id}")

        return dict(row)

    async def validate_accounts(self, account_ids: List[str], platform_code: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        éªŒè¯è´¦å·æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        å¦‚æœ platform_code ä¸º Noneï¼Œåˆ™ä¸éªŒè¯å¹³å°åŒ¹é…ï¼ˆç”¨äºå¤šå¹³å°å‘å¸ƒï¼‰
        """
        valid_accounts = []

        for account_id in account_ids:
            account = cookie_manager.get_account_by_id(account_id)
            if not account:
                raise NotFoundException(f"è´¦å·ä¸å­˜åœ¨: {account_id}")

            # æ£€æŸ¥ cookie_file æ˜¯å¦å­˜åœ¨
            if not account.get('cookie_file'):
                logger.error(f"è´¦å· {account_id} çš„ cookie_file ä¸ºç©º")
                raise BadRequestException(
                    f"è´¦å· {account_id} çš„ Cookie æ–‡ä»¶è·¯å¾„ä¸ºç©ºï¼Œæ— æ³•å‘å¸ƒã€‚"
                    f"è¯·é‡æ–°å¯¼å…¥è¯¥è´¦å·æˆ–è”ç³»ç®¡ç†å‘˜ã€‚"
                )

            # å¦‚æœæŒ‡å®šäº†å¹³å°ï¼Œæ£€æŸ¥å¹³å°æ˜¯å¦åŒ¹é…
            if platform_code is not None and account.get('platform_code') != platform_code:
                raise BadRequestException(
                    f"è´¦å· {account_id} å¹³å°ä¸åŒ¹é… (æœŸæœ›: {self.PLATFORM_MAP[platform_code]}, "
                    f"å®é™…: {account.get('platform')})"
                )

            # æ£€æŸ¥è´¦å·çŠ¶æ€
            if account.get('status') != 'valid':
                logger.warning(f"è´¦å· {account_id} çŠ¶æ€ä¸º {account.get('status')}, å¯èƒ½æ— æ³•å‘å¸ƒ")

            logger.info(f"âœ… éªŒè¯è´¦å·: {account_id} - Cookieæ–‡ä»¶: {account.get('cookie_file')}")
            valid_accounts.append(account)

        return valid_accounts

    async def publish_batch(
        self,
        db,
        file_ids: List[int],
        accounts: List[str],
        platform: Optional[int] = None,
        title: str = "",
        description: Optional[str] = None,
        topics: Optional[List[str]] = None,
        cover_path: Optional[str] = None,
        scheduled_time: Optional[str] = None,
        interval_control_enabled: bool = False,
        interval_mode: Optional[str] = None,
        interval_seconds: Optional[int] = 300,
        priority: int = 5,
        items: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡å‘å¸ƒ
        æ”¯æŒå•å¹³å°å’Œå¤šå¹³å°å‘å¸ƒ
        - å¦‚æœæŒ‡å®šäº† platformï¼Œåˆ™åªå‘å¸ƒåˆ°è¯¥å¹³å°ï¼ˆéªŒè¯è´¦å·å¿…é¡»å±äºè¯¥å¹³å°ï¼‰
        - å¦‚æœæœªæŒ‡å®š platformï¼Œåˆ™æ”¯æŒå¤šå¹³å°å‘å¸ƒï¼ˆè‡ªåŠ¨æŒ‰è´¦å·å¹³å°åˆ†ç»„ï¼‰
        """
        batch_id = f"batch_{uuid.uuid4().hex[:12]}"
        results = {
            "batch_id": batch_id,
            "total_tasks": 0,
            "success_count": 0,
            "failed_count": 0,
            "pending_count": 0,
            "tasks": []
        }

        # å®šæ—¶å‘å¸ƒï¼šæ‰¹é‡ä»»åŠ¡ä¼šç«‹å³æ‰§è¡Œï¼Œä½†åœ¨å¹³å°ä¾§è®¾ç½®â€œå®šæ—¶å‘å¸ƒâ€æ—¶é—´
        timer_config = None
        if scheduled_time:
            timer_config = self._parse_schedule_time(scheduled_time)
            logger.info(f"æ‰¹é‡å®šæ—¶å‘å¸ƒé…ç½®: {timer_config}")

        # éªŒè¯è´¦å·ï¼ˆå¦‚æœæŒ‡å®šäº†å¹³å°åˆ™éªŒè¯å¹³å°åŒ¹é…ï¼Œå¦åˆ™ä¸éªŒè¯ï¼‰
        try:
            valid_accounts = await self.validate_accounts(accounts, platform)
        except Exception as e:
            logger.error(f"æ‰¹é‡å‘å¸ƒè´¦å·éªŒè¯å¤±è´¥: {e}")
            raise

        # å¦‚æœæ˜¯å¤šå¹³å°å‘å¸ƒï¼ŒæŒ‰å¹³å°åˆ†ç»„è´¦å·
        if platform is None:
            # æŒ‰å¹³å°åˆ†ç»„
            from collections import defaultdict
            accounts_by_platform = defaultdict(list)
            for account in valid_accounts:
                acc_platform = account.get('platform_code')
                if acc_platform:
                    accounts_by_platform[acc_platform].append(account)

            logger.info(f"å¤šå¹³å°å‘å¸ƒ: æ£€æµ‹åˆ° {len(accounts_by_platform)} ä¸ªå¹³å°")

            # ä¸ºæ¯ä¸ªå¹³å°åˆ›å»ºä»»åŠ¡
            for plat_code, plat_accounts in accounts_by_platform.items():
                await self._create_batch_tasks(
                    db, batch_id, file_ids, plat_accounts, plat_code,
                    title,
                    description,
                    topics,
                    cover_path,
                    priority,
                    items,
                    results,
                    timer_config,
                    interval_control_enabled=interval_control_enabled,
                    interval_mode=interval_mode,
                    interval_seconds=interval_seconds,
                )
        else:
            # å•å¹³å°å‘å¸ƒ
            await self._create_batch_tasks(
                db, batch_id, file_ids, valid_accounts, platform,
                title,
                description,
                topics,
                cover_path,
                priority,
                items,
                results,
                timer_config,
                interval_control_enabled=interval_control_enabled,
                interval_mode=interval_mode,
                interval_seconds=interval_seconds,
            )

        logger.info(
            f"æ‰¹é‡å‘å¸ƒä»»åŠ¡åˆ›å»ºå®Œæˆ: batch_id={batch_id}, "
            f"æˆåŠŸ={results['success_count']}, å¤±è´¥={results['failed_count']}"
        )

        return results

    async def _create_batch_tasks(
        self,
        db,
        batch_id: str,
        file_ids: List[int],
        accounts: List[Dict[str, Any]],
        platform: int,
        title: str,
        description: Optional[str],
        topics: Optional[List[str]],
        cover_path: Optional[str],
        priority: int,
        items: Optional[List[Dict[str, Any]]],
        results: Dict[str, Any],
        timer_config: Optional[Dict[str, Any]] = None,
        *,
        interval_control_enabled: bool = False,
        interval_mode: Optional[str] = None,
        interval_seconds: Optional[int] = 300,
    ):
        """åˆ›å»ºæ‰¹é‡å‘å¸ƒä»»åŠ¡çš„å†…éƒ¨æ–¹æ³•"""
        base_time = datetime.now()
        interval_s = int(interval_seconds or 0)
        mode = (interval_mode or "").strip()
        # ä¸ºæ¯ä¸ªæ–‡ä»¶å’Œæ¯ä¸ªè´¦å·åˆ›å»ºç‹¬ç«‹ä»»åŠ¡
        for file_idx, file_id in enumerate(file_ids):
            try:
                # éªŒè¯æ–‡ä»¶
                file_record = await self.validate_file(db, file_id)

                # Material metadata (fromç´ æåº“), used as defaults when request doesn't override.
                stored_title = file_record.get("ai_title") or file_record.get("title")
                stored_desc = file_record.get("ai_description") or file_record.get("description")
                stored_cover = file_record.get("cover_image") or file_record.get("cover")

                stored_tags: List[str] = []
                raw_tags_field = file_record.get("tags")
                if raw_tags_field:
                    try:
                        if isinstance(raw_tags_field, str):
                            parsed = json.loads(raw_tags_field)
                            if isinstance(parsed, list):
                                stored_tags = [str(t) for t in parsed if str(t).strip()]
                            else:
                                # Accept common separators (comma/space/Chinese comma) and strip leading '#'
                                parts = [p for p in re.split(r"[\s,ï¼Œ]+", raw_tags_field) if p and p.strip()]
                                stored_tags = [p.lstrip("#").strip() for p in parts if p.lstrip("#").strip()]
                        elif isinstance(raw_tags_field, list):
                            stored_tags = [str(t) for t in raw_tags_field if str(t).strip()]
                    except Exception:
                        parts = [p for p in re.split(r"[\s,ï¼Œ]+", str(raw_tags_field)) if p and p.strip()]
                        stored_tags = [p.lstrip("#").strip() for p in parts if p.lstrip("#").strip()]

                parsed_ai_tags: List[str] = []
                if file_record.get('ai_tags'):
                    try:
                        raw_tags = file_record.get('ai_tags')
                        parsed = json.loads(raw_tags) if isinstance(raw_tags, str) else raw_tags
                        if isinstance(parsed, list):
                            parsed_ai_tags = [str(tag) for tag in parsed if str(tag).strip()]
                    except Exception as e:
                        logger.warning(f"Failed to parse ai_tags for file {file_id}: {e}")

                # æŸ¥æ‰¾æ˜¯å¦æœ‰ç‹¬ç«‹é…ç½®
                item_config = next((i for i in (items or []) if (i.file_id if hasattr(i, 'file_id') else i.get('file_id')) == file_id), None)

                # è°ƒè¯•æ—¥å¿—
                logger.info(f"ğŸ“ [Publish Debug] file_id={file_id}")
                logger.info(f"ğŸ“ [Publish Debug] item_config={item_config}")
                logger.info(f"ğŸ“ [Publish Debug] global title={title}")
                logger.info(f"ğŸ“ [Publish Debug] global description={description}")

                # ç¡®å®šæœ€ç»ˆå‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ item_configï¼Œå¦åˆ™ä½¿ç”¨ç»Ÿä¸€å‚æ•°ï¼‰
                # æ”¯æŒ Pydantic æ¨¡å‹å’Œå­—å…¸ä¸¤ç§æ ¼å¼
                if item_config:
                    if hasattr(item_config, 'title'):
                        # Pydantic æ¨¡å‹
                        final_title = item_config.title if item_config.title else (title or stored_title or Path(file_record['file_path']).stem)
                        final_desc = item_config.description if item_config.description is not None else (description or stored_desc or "")
                        final_topics = item_config.topics if item_config.topics is not None else (topics or stored_tags or [])
                        final_cover = item_config.cover_path if item_config.cover_path else (cover_path or stored_cover or "")
                    else:
                        # å­—å…¸æ ¼å¼
                        final_title = item_config.get('title') if item_config.get('title') else (title or stored_title or Path(file_record['file_path']).stem)
                        final_desc = item_config.get('description') if item_config.get('description') is not None else (description or stored_desc or "")
                        final_topics = item_config.get('topics') if item_config.get('topics') is not None else (topics or stored_tags or [])
                        final_cover = item_config.get('cover_path') if item_config.get('cover_path') else (cover_path or stored_cover or "")
                else:
                    final_title = title or stored_title or Path(file_record['file_path']).stem
                    final_desc = description or stored_desc or ""
                    final_topics = topics or stored_tags or []
                    final_cover = cover_path or stored_cover or ""

                if (not final_topics) and parsed_ai_tags:
                    final_topics = parsed_ai_tags

                logger.info(f"âœ… [Publish Debug] final_title={final_title}")
                logger.info(f"âœ… [Publish Debug] final_desc={final_desc}")
                logger.info(f"âœ… [Publish Debug] final_topics={final_topics}")

                # ğŸ†• ä½¿ç”¨å¹³å°é€‚é…å™¨æ ¼å¼åŒ–å…ƒæ•°æ®
                formatted_metadata = format_metadata_for_platform(
                    platform_code=platform,
                    metadata={
                        "title": final_title,
                        "description": final_desc,
                        "topics": final_topics
                    }
                )
                logger.info(f"ğŸ¯ [Platform Adapter] Formatted metadata for platform {platform}: {formatted_metadata}")

                # ä¸ºæ¯ä¸ªè´¦å·åˆ›å»ºç‹¬ç«‹ä»»åŠ¡
                for account_idx, account in enumerate(accounts):
                    # è°ƒè¯•ï¼šè®°å½•æ–‡ä»¶è·¯å¾„ä¿¡æ¯
                    raw_file_path = file_record.get("file_path") or ""
                    portable_path = self._portable_video_path(raw_file_path)
                    logger.info(f"[PublishService] file_id={file_id}, raw_path={raw_file_path}, portable_path={portable_path}")
                    logger.info(f"[PublishService] Path exists: {Path(portable_path).exists() if portable_path else False}")

                    # åˆ›å»ºä»»åŠ¡æ•°æ®ï¼ˆå•ä¸ªè´¦å·ï¼‰ï¼Œä½¿ç”¨æ ¼å¼åŒ–åçš„å…ƒæ•°æ®
                    task_data = {
                        "batch_id": batch_id,
                        "file_id": file_id,
                        "video_path": portable_path,
                        "account_id": account['account_id'],
                        "account_name": account.get("original_name") or account.get("name") or account.get("account_id"),
                        "cookie_file": account['cookie_file'],
                        "platform": platform,
                        "title": formatted_metadata.get("title", final_title),
                        "description": formatted_metadata.get("description", final_desc),
                        "tags": formatted_metadata.get("tags", final_topics),
                        "publish_date": (timer_config or {}).get("scheduled_time") or 0,
                        "thumbnail_path": final_cover or "",
                    }

                    # Optional interval control: delay task execution by setting `not_before`.
                    if interval_control_enabled and interval_s > 0 and mode in ("account_first", "video_first"):
                        if mode == "video_first":
                            offset = file_idx * interval_s
                        else:
                            # account_first: stagger accounts and keep each account's files sequential.
                            offset = (account_idx * interval_s) + (file_idx * interval_s * max(len(accounts), 1))
                        task_data["not_before"] = (base_time + timedelta(seconds=offset)).isoformat()

                    task_id = f"publish_{batch_id}_{file_id}_{account['account_id']}"
                    task = Task(
                        task_id=task_id,
                        task_type=TaskType.PUBLISH,
                        data=task_data,
                        priority=priority,
                        max_retries=0  # ç¦ç”¨è‡ªåŠ¨é‡è¯•ï¼Œå¤±è´¥ä»»åŠ¡éœ€æ‰‹åŠ¨å¤„ç†
                    )

                    success = self.task_manager.add_task(task)
                    results["total_tasks"] += 1

                    if success:
                        results["success_count"] += 1
                        results["pending_count"] += 1
                        results["tasks"].append({
                            "task_id": task_id,
                            "file_id": file_id,
                            "platform": platform,
                            "account_id": account['account_id'],
                            "status": "pending"
                        })
                    else:
                        results["failed_count"] += 1
                        results["tasks"].append({
                            "task_id": task_id,
                            "file_id": file_id,
                            "platform": platform,
                            "account_id": account['account_id'],
                            "status": "failed",
                            "error_message": "æ— æ³•æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—"
                        })

            except Exception as e:
                logger.error(f"æ‰¹é‡å‘å¸ƒæ–‡ä»¶ {file_id} å¤±è´¥: {e}")
                results["failed_count"] += 1
                results["total_tasks"] += 1
                results["tasks"].append({
                    "task_id": f"failed_{file_id}",
                    "file_id": file_id,
                    "platform": platform,
                    "status": "failed",
                    "error_message": str(e)
                })

    async def list_presets(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å‘å¸ƒé¢„è®¾"""
        loop = asyncio.get_event_loop()
        presets = await loop.run_in_executor(
            self.executor,
            self.preset_manager.get_all_presets
        )
        return presets

    async def create_preset(self, preset_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºå‘å¸ƒé¢„è®¾"""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor,
            self.preset_manager.create_preset,
            preset_data
        )

        if not result.get('success'):
            raise BadRequestException(f"åˆ›å»ºé¢„è®¾å¤±è´¥: {result.get('error')}")

        logger.info(f"é¢„è®¾åˆ›å»ºæˆåŠŸ: {preset_data.get('name')} (ID: {result.get('id')})")
        return result

    async def update_preset(self, preset_id: int, preset_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ›´æ–°å‘å¸ƒé¢„è®¾"""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor,
            self.preset_manager.update_preset,
            preset_id,
            preset_data
        )

        if not result.get('success'):
            raise BadRequestException(f"æ›´æ–°é¢„è®¾å¤±è´¥: {result.get('error')}")

        logger.info(f"é¢„è®¾æ›´æ–°æˆåŠŸ: ID {preset_id}")
        return result

    async def delete_preset(self, preset_id: int) -> Dict[str, Any]:
        """åˆ é™¤å‘å¸ƒé¢„è®¾"""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor,
            self.preset_manager.delete_preset,
            preset_id
        )

        if not result.get('success'):
            raise NotFoundException(f"é¢„è®¾ä¸å­˜åœ¨: ID {preset_id}")

        logger.info(f"é¢„è®¾åˆ é™¤æˆåŠŸ: ID {preset_id}")
        return result

    async def use_preset(
        self,
        db,
        preset_id: int,
        file_ids: List[int],
        override_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨é¢„è®¾å‘å¸ƒ
        æ”¯æŒè¦†ç›–é¢„è®¾ä¸­çš„éƒ¨åˆ†å‚æ•°
        """
        # è·å–é¢„è®¾
        loop = asyncio.get_event_loop()
        presets = await loop.run_in_executor(
            self.executor,
            self.preset_manager.get_all_presets
        )

        preset = next((p for p in presets if p['id'] == preset_id), None)
        if not preset:
            raise NotFoundException(f"é¢„è®¾ä¸å­˜åœ¨: ID {preset_id}")

        # å¢åŠ ä½¿ç”¨æ¬¡æ•°
        await loop.run_in_executor(
            self.executor,
            self.preset_manager.increment_usage,
            preset_id
        )

        # æ„å»ºå‘å¸ƒå‚æ•°
        publish_params = {
            "file_ids": file_ids,
            "accounts": preset.get('accounts', []),
            "platform": preset.get('platform'),
            "title": preset.get('default_title') or preset.get('title', ''),
            "description": preset.get('description', ''),
            "topics": preset.get('tags', [])
        }

        # åº”ç”¨è¦†ç›–å‚æ•°
        if override_data:
            publish_params.update(override_data)

        # æ‰¹é‡å‘å¸ƒ
        result = await self.publish_batch(db, **publish_params)

        logger.info(f"ä½¿ç”¨é¢„è®¾å‘å¸ƒ: preset_id={preset_id}, batch_id={result.get('batch_id')}")
        return result

    async def get_publish_history(
        self,
        db,
        platform: Optional[int] = None,
        status: Optional[str] = None,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """è·å–å‘å¸ƒå†å²"""
        if mysql_enabled():
            warnings.warn("SQLite publish_tasks path is deprecated; using MySQL via DATABASE_URL", DeprecationWarning)
            where = ["1=1"]
            params: dict = {"limit": int(limit)}

            if platform:
                where.append("platform = :platform")
                params["platform"] = str(platform)
            if status:
                where.append("status = :status")
                params["status"] = status

            where_sql = " AND ".join(where)
            with sa_connection() as conn:
                rows = conn.execute(
                    text(f"SELECT * FROM publish_tasks WHERE {where_sql} ORDER BY created_at DESC LIMIT :limit"),
                    params,
                ).mappings().all()
            return [dict(r) for r in rows]

        cursor = db.cursor()

        query = "SELECT * FROM publish_tasks WHERE 1=1"
        params = []

        if platform:
            query += " AND platform = ?"
            params.append(str(platform))

        if status:
            query += " AND status = ?"
            params.append(status)

        query += " ORDER BY created_at DESC LIMIT ?"
        params.append(limit)

        cursor.execute(query, params)
        rows = cursor.fetchall()

        history = []
        for row in rows:
            history.append(dict(row))

        return history

    def _parse_schedule_time(self, scheduled_time: str) -> Dict[str, Any]:
        """
        è§£æå®šæ—¶å‘å¸ƒæ—¶é—´
        æ ¼å¼: "YYYY-MM-DD HH:MM" æˆ– "HH:MM"
        """
        try:
            # å°è¯•è§£æå®Œæ•´æ—¥æœŸæ—¶é—´
            if " " in scheduled_time:
                target_time = datetime.strptime(scheduled_time, "%Y-%m-%d %H:%M")
            else:
                # åªæœ‰æ—¶é—´ï¼Œä½¿ç”¨ä»Šå¤©æˆ–æ˜å¤©çš„æ—¥æœŸ
                time_obj = datetime.strptime(scheduled_time, "%H:%M")
                now = datetime.now()
                target_time = now.replace(
                    hour=time_obj.hour,
                    minute=time_obj.minute,
                    second=0,
                    microsecond=0
                )

                # å¦‚æœæ—¶é—´å·²è¿‡ï¼Œè®¾ç½®ä¸ºæ˜å¤©
                if target_time < now:
                    target_time += timedelta(days=1)

            # è®¡ç®—å»¶è¿Ÿç§’æ•°
            delay_seconds = (target_time - datetime.now()).total_seconds()

            if delay_seconds < 0:
                raise ValueError("å®šæ—¶æ—¶é—´ä¸èƒ½æ—©äºå½“å‰æ—¶é—´")

            return {
                "scheduled_time": target_time.isoformat(),
                "delay_seconds": int(delay_seconds),
                "enable_timer": True
            }

        except ValueError as e:
            raise BadRequestException(f"å®šæ—¶æ—¶é—´æ ¼å¼é”™è¯¯: {str(e)}")


# å…¨å±€æœåŠ¡å®ä¾‹å·¥å‚
def get_publish_service(task_manager: TaskQueueManager) -> PublishService:
    """è·å–å‘å¸ƒæœåŠ¡å®ä¾‹"""
    return PublishService(task_manager)
