"""
è§†é¢‘å·æ•°æ®é‡‡é›†æµ‹è¯•è„šæœ¬
æµ‹è¯•å®Œæ•´çš„æŠ“å–æµç¨‹
"""
import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from crawlers.wechat_channels.channels_crawler import WechatChannelsCrawler
from crawlers.wechat_channels.storage import get_wechat_channels_storage
from loguru import logger


async def test_basic_crawl():
    """æµ‹è¯•åŸºç¡€çˆ¬å–åŠŸèƒ½ï¼ˆä¸ä½¿ç”¨ AIï¼‰"""
    logger.info("=" * 50)
    logger.info("æµ‹è¯• 1: åŸºç¡€çˆ¬å–åŠŸèƒ½")
    logger.info("=" * 50)

    # ä½¿ç”¨ç¬¬ä¸€ä¸ª Cookie æ–‡ä»¶
    cookie_file = "channels_sphjscpkHodFygw.json"

    crawler = WechatChannelsCrawler()
    result = await crawler.start(
        account_cookie_file=cookie_file,
        max_pages=1  # åªæŠ“å–ç¬¬ä¸€é¡µç”¨äºæµ‹è¯•
    )

    if result["success"]:
        logger.success(f"âœ… åŸºç¡€çˆ¬å–æˆåŠŸï¼")
        logger.info(f"è´¦å·ID: {result['data'].get('account_id')}")
        logger.info(f"è´¦å·åç§°: {result['data'].get('account_name')}")
        logger.info(f"è§†é¢‘æ•°é‡: {result['data']['total']}")

        # æ˜¾ç¤ºå‰ 3 ä¸ªè§†é¢‘
        videos = result["data"]["videos"]
        for i, video in enumerate(videos[:3], 1):
            logger.info(f"\nè§†é¢‘ {i}:")
            logger.info(f"  æ ‡é¢˜: {video.get('title', 'N/A')}")
            logger.info(f"  å°é¢: {video.get('cover_url', 'N/A')[:50]}...")
            logger.info(f"  ç»Ÿè®¡: {video.get('stats', 'N/A')}")
    else:
        logger.error(f"âŒ åŸºç¡€çˆ¬å–å¤±è´¥: {result.get('error')}")

    return result


async def test_with_storage():
    """æµ‹è¯•æ•°æ®å­˜å‚¨åŠŸèƒ½"""
    logger.info("\n" + "=" * 50)
    logger.info("æµ‹è¯• 2: æ•°æ®å­˜å‚¨åŠŸèƒ½")
    logger.info("=" * 50)

    cookie_file = "channels_sphjscpkHodFygw.json"

    # åˆå§‹åŒ–å­˜å‚¨
    storage = get_wechat_channels_storage()

    # åˆ›å»ºæŠ“å–ä»»åŠ¡
    task_id = storage.create_crawl_task({
        "account_id": "sphjscpkHodFygw",
        "task_type": "fetch_videos",
        "max_pages": 1
    })
    logger.info(f"âœ… åˆ›å»ºæŠ“å–ä»»åŠ¡ï¼ŒID: {task_id}")

    # æ‰§è¡Œçˆ¬å–
    crawler = WechatChannelsCrawler()
    result = await crawler.start(cookie_file, max_pages=1)

    if result["success"]:
        videos = result["data"]["videos"]

        # ä¸ºè§†é¢‘æ·»åŠ  account_id
        for video in videos:
            video["account_id"] = result["data"].get("account_id")

        # ä¿å­˜åˆ°æ•°æ®åº“
        saved_count = storage.save_videos_batch(videos)
        logger.success(f"âœ… å·²ä¿å­˜ {saved_count} ä¸ªè§†é¢‘åˆ°æ•°æ®åº“")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        storage.update_crawl_task(task_id, status="completed", videos_count=saved_count)
        logger.success(f"âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°")

        # ä»æ•°æ®åº“è¯»å–
        db_videos = storage.get_videos_by_account("sphjscpkHodFygw", limit=5)
        logger.info(f"ğŸ“š ä»æ•°æ®åº“è¯»å–äº† {len(db_videos)} ä¸ªè§†é¢‘")

    else:
        storage.update_crawl_task(
            task_id,
            status="failed",
            error_message=result.get("error", "æœªçŸ¥é”™è¯¯")
        )
        logger.error(f"âŒ çˆ¬å–å¤±è´¥: {result.get('error')}")


async def test_api_endpoint():
    """æµ‹è¯• API æ¥å£"""
    logger.info("\n" + "=" * 50)
    logger.info("æµ‹è¯• 3: API æ¥å£è°ƒç”¨")
    logger.info("=" * 50)

    try:
        import httpx

        async with httpx.AsyncClient(timeout=120.0) as client:
            # æµ‹è¯•å¥åº·æ£€æŸ¥
            logger.info("ğŸ” æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£...")
            response = await client.get("http://localhost:7000/api/v1/wechat-channels/health")
            logger.info(f"å¥åº·æ£€æŸ¥: {response.json()}")

            # æµ‹è¯•æŠ“å–æ¥å£
            logger.info("\nğŸ” æµ‹è¯•æŠ“å–æ¥å£...")
            response = await client.post(
                "http://localhost:7000/api/v1/wechat-channels/fetch-videos",
                json={
                    "account_cookie_file": "channels_sphjscpkHodFygw.json",
                    "max_pages": 1,
                    "use_ai_enhance": False
                }
            )

            if response.status_code == 200:
                data = response.json()
                if data["success"]:
                    logger.success(f"âœ… API è°ƒç”¨æˆåŠŸï¼")
                    logger.info(f"è§†é¢‘æ•°é‡: {data['data']['total']}")
                    logger.info(f"å·²ä¿å­˜: {data['data'].get('saved_count', 0)} ä¸ª")
                else:
                    logger.error(f"âŒ API è¿”å›é”™è¯¯: {data.get('error')}")
            else:
                logger.error(f"âŒ HTTP é”™è¯¯: {response.status_code}")
                logger.error(f"å“åº”: {response.text}")

    except httpx.ConnectError:
        logger.warning("âš ï¸  FastAPI æœåŠ¡æœªè¿è¡Œï¼Œè·³è¿‡ API æµ‹è¯•")
        logger.info("æç¤º: è¯·å…ˆè¿è¡Œ FastAPI æœåŠ¡: python fastapi_app/main.py")
    except Exception as e:
        logger.error(f"âŒ API æµ‹è¯•å¤±è´¥: {e}")


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    logger.info("ğŸš€ å¼€å§‹è§†é¢‘å·æ•°æ®é‡‡é›†åŠŸèƒ½æµ‹è¯•\n")

    # æµ‹è¯• 1: åŸºç¡€çˆ¬å–
    await test_basic_crawl()

    # æµ‹è¯• 2: æ•°æ®å­˜å‚¨
    await test_with_storage()

    # æµ‹è¯• 3: API æ¥å£ï¼ˆéœ€è¦ FastAPI æœåŠ¡è¿è¡Œï¼‰
    await test_api_endpoint()

    logger.info("\n" + "=" * 50)
    logger.success("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    logger.info("=" * 50)


if __name__ == "__main__":
    asyncio.run(main())
